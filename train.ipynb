{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from model import llm, Mllm\n",
    "from BPE import BPE_Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1\n",
    "# os.environ[\"TORCH_LOGS\"] = \"+dynamo\"\n",
    "# os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=128\n",
    "BATCH_SIZE=64\n",
    "model_folder = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset split files\n",
    "# check if the dataset is already split\n",
    "\n",
    "# if os.path.exists('data/train.txt') and os.path.exists('data/test.txt'):\n",
    "#     print('Dataset split files already exist')\n",
    "# else:\n",
    "#     print('Dataset split files do not exist')\n",
    "#     print('Creating dataset split files...')\n",
    "#     with open('data/Harry_Potter_all_books_preprocessed.txt', 'r') as f:\n",
    "#         full = f.read()\n",
    "#         train = full[:int(len(full)*0.8)]\n",
    "#         test = full[int(len(full)*0.8):]\n",
    "#         print('Train size:', len(train))\n",
    "#         print('Test size:', len(test))\n",
    "#     with open('data/train.txt', 'w') as f:\n",
    "#         f.write(train)\n",
    "#     with open('data/test.txt', 'w') as f:\n",
    "#         f.write(test)\n",
    "#     print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, tokenizer, block_size, divider=2):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.data = []\n",
    "        self.divider = divider\n",
    "        with open(data_path, 'r') as f:\n",
    "            full_text = f.read()\n",
    "            full_text = full_text.replace('.', ' . ')\n",
    "            full_text = full_text.replace(',', ' , ')\n",
    "            full_text = full_text.replace('!', ' ! ')\n",
    "            full_text = full_text.replace('?', ' ? ')\n",
    "            full_text = full_text.replace(':', ' : ')\n",
    "            full_text = full_text.replace(';', ' ; ')\n",
    "            full_text = full_text.replace(')', '')\n",
    "            full_text = full_text.replace('(', '')\n",
    "            full_text = full_text.replace('@', '')\n",
    "            full_text = full_text.replace('|', '')\n",
    "            full_text = full_text.replace(']', '')\n",
    "            full_text = full_text.replace('[', '')\n",
    "            full_text = full_text.replace('~', '')\n",
    "            full_text = full_text.replace('^', '')\n",
    "            full_text = full_text.replace('<', '')\n",
    "            full_text = full_text.replace('>', '')\n",
    "            full_text = full_text.replace('&', '')\n",
    "            full_text = full_text.replace('{', '')\n",
    "            full_text = full_text.replace('}', '')\n",
    "            full_text = full_text.replace('+', '')\n",
    "            # full_text = full_text.replace('-', '')\n",
    "            full_text = full_text.replace('tititi', '')\n",
    "            full_text = full_text.replace('orerer', '')\n",
    "            full_text = full_text.replace('errero', '')\n",
    "            full_text = full_text.replace('\\u007f', '')\n",
    "            full_text = full_text.replace('_', '')\n",
    "            full_text = full_text.replace('%', '')\n",
    "            full_text = full_text.replace('$', '')\n",
    "            full_text = full_text.replace('\\\\', '')\n",
    "            full_text = full_text.replace('=', '')\n",
    "            full_text = full_text.replace('#', '')\n",
    "            full_text = full_text.replace(';', '')\n",
    "            full_text = full_text.replace(':', '')\n",
    "            full_text = full_text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "            # self.data = full_text.split()\n",
    "            tokenized = tokenizer.tokenize(full_text)\n",
    "            # break into blocks\n",
    "            for i in range(0, len(tokenized) - block_size + 1, block_size//divider):\n",
    "                self.data.append(tokenized[i:i+block_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.data[idx][:self.block_size-1], dtype=torch.long)\n",
    "        Y = torch.tensor(self.data[idx][1:self.block_size], dtype=torch.long)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = BPE_Tokenizer()\n",
    "tokenizer.load_vocab('openweb+Nicolas.json')\n",
    "\n",
    "# create dataset\n",
    "print('Creating dataset...')\n",
    "print('loading Train')\n",
    "train_set = TokenDataset('data/train.txt', tokenizer, BLOCK_SIZE, divider=2)\n",
    "print('loading Test')\n",
    "test_set = TokenDataset('data/test.txt', tokenizer, BLOCK_SIZE,divider=2)\n",
    "\n",
    "print('loading Tune Train')\n",
    "tune_train_set = TokenDataset('data/nicolasSTASTrain.txt', tokenizer, BLOCK_SIZE, divider=4)\n",
    "print('loading Tune Test')\n",
    "tune_test_set = TokenDataset('data/nicolasSTASTest.txt', tokenizer, BLOCK_SIZE, divider=4)\n",
    "\n",
    "print('sample from train set:', len(train_set[0]), tokenizer.detokenize(train_set[0][0].tolist()))\n",
    "print('sample from test set:', len(test_set[0]), tokenizer.detokenize(test_set[0][0].tolist()))\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "tune_train_loader = DataLoader(tune_train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "tune_test_loader = DataLoader(tune_test_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "#  check that dataloader works\n",
    "for batch in train_loader:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  init model\n",
    "# N_EMBED = VOCAB_SIZE*2\n",
    "N_EMBED = 256\n",
    "N_HEAD = 4\n",
    "N_LAYERS = 6\n",
    "# model = llm(VOCAB_SIZE, BLOCK_SIZE, N_EMBED, N_HEAD, N_LAYERS)\n",
    "model = Mllm(VOCAB_SIZE, BLOCK_SIZE, N_EMBED, N_HEAD, N_LAYERS)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def speak(model, tokenizer, prompt, max_len=100, temperature=0.5, repetition = 1.1):\n",
    "    input_ = tokenizer.tokenize(prompt)\n",
    "    gen = model.generate(torch.tensor(input_).unsqueeze(0).to(device), max_len, temperature=temperature, repetition_penalty=repetition, top_k=10)\n",
    "    text = tokenizer.detokenize(gen.cpu().squeeze(0).tolist()) \n",
    "    text = text.replace(' . ', '.\\n')\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(testlodaer, trainloader):\n",
    "    model.eval()\n",
    "    # losses = torch.zeros((BATCH_SIZE,BLOCK_SIZE))\n",
    "    test_losses = []\n",
    "    for k, (X,Y) in enumerate(tqdm(testlodaer, desc='eval test')):\n",
    "        X,Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        test_losses.append(loss.item())\n",
    "    train_losses = []\n",
    "    for k, (X,Y) in enumerate(tqdm(trainloader, desc='eval train')):\n",
    "        X,Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        train_losses.append(loss.item())\n",
    "    model.train()\n",
    "    return {'test': np.array(test_losses).mean(), 'train':np.array(train_losses).mean()}\n",
    "\n",
    "# estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 0.001\n",
    "LEARNING_RATE = 3e-4\n",
    "MIN_LR = 6e-5\n",
    "WARMUP_STEPS = 2\n",
    "LR_DECAY_ITERS = 20\n",
    "MAX_ITERS = 120\n",
    "# LEARNING_RATE = 1e-3\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for WARMUP_STEPS steps\n",
    "    if it < WARMUP_STEPS:\n",
    "        return LEARNING_RATE * it / WARMUP_STEPS\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > LR_DECAY_ITERS:\n",
    "        return MIN_LR\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - WARMUP_STEPS) / (LR_DECAY_ITERS - WARMUP_STEPS)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = model.configure_optimizers(WEIGHT_DECAY, LEARNING_RATE, (beta1, beta2),'cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(beta1, beta2), weight_decay=WEIGHT_DECAY)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compile model\n",
    "print('Compiling model... (takes a while)')\n",
    "# model = model.to('cuda')\n",
    "model = torch.compile(model).to(device)\n",
    "# # model.train()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "# writer.add_graph(model, torch.zeros((BATCH_SIZE, BLOCK_SIZE)).to(device))\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for e in range(MAX_ITERS):\n",
    "    losses = estimate_loss(test_loader, train_loader)\n",
    "    print('Epoch', e, 'Losses:', losses)\n",
    "    speak(model, tokenizer, \"This morning on radio news\", max_len=256)\n",
    "    lr = get_lr(e)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    if losses['test'] < best_loss:\n",
    "        best_loss = losses['test']\n",
    "        torch.save(model.state_dict(), f'{model_folder}/best_llm.pt')\n",
    "        print('Saved model')\n",
    "    writer.add_scalar('loss/train', losses['train'], e)\n",
    "    writer.add_scalar('loss/test', losses['test'], e)\n",
    "    writer.add_scalar('lr', lr, e)\n",
    "    for k, (X,Y) in enumerate(tqdm(train_loader, desc='train')):\n",
    "        X,Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{model_folder}/best_llm.pt'))\n",
    "model.eval()\n",
    "speak(model, tokenizer, \"They are searching for \", max_len=2048)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tune \n",
    "LEARNING_RATE = 6e-6\n",
    "MIN_LR = 5e-7\n",
    "WARMUP_STEPS = 10\n",
    "LR_DECAY_ITERS = 100\n",
    "MAX_ITERS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# writer = SummaryWriter()\n",
    "# writer.add_graph(model, torch.zeros((BATCH_SIZE, BLOCK_SIZE)).cuda())\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for e in range(MAX_ITERS):\n",
    "    losses = estimate_loss(tune_test_loader, tune_train_loader)\n",
    "    print('Epoch', e, 'Losses:', losses)\n",
    "    speak(model, tokenizer, \"Hi there! I'm Nicolas\", max_len=50)\n",
    "    lr = get_lr(e)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    if losses['test'] < best_loss:\n",
    "        best_loss = losses['test']\n",
    "        torch.save(model.state_dict(), f'{model_folder}/best_tune_llm.pt')\n",
    "        print('Saved model')\n",
    "    if e % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'{model_folder}/tune_llm_{e}.pt')\n",
    "        print('Saved model')\n",
    "    writer.add_scalar('loss/train', losses['train'], e)\n",
    "    writer.add_scalar('loss/test', losses['test'], e)\n",
    "    writer.add_scalar('lr', lr, e)\n",
    "    for k, (X,Y) in enumerate(tqdm(tune_train_loader, desc='train')):\n",
    "        X,Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('small/best_tune_llm.pt'))\n",
    "# modle = torch.compile(model).to('cuda')\n",
    "model.load_state_dict(torch.load(f'{model_folder}/best_tune_llm.pt'))\n",
    "model.eval()\n",
    "speak(model, tokenizer, \"Hi there! I'm Nicolas STAS\", max_len=1024, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix) :]\n",
    "    return text\n",
    "\n",
    "\n",
    "def repair_checkpoint(path, out_path=\"fixed.pt\"):\n",
    "    ckpt = torch.load(path)\n",
    "    # print(ckpt.keys())\n",
    "    # in_state_dict = ckpt[\"model_state_dict\"]\n",
    "    in_state_dict = ckpt\n",
    "    pairings = [\n",
    "        (src_key, remove_prefix(src_key, \"_orig_mod.\"))\n",
    "        for src_key in in_state_dict.keys()\n",
    "    ]\n",
    "    if all(src_key == dest_key for src_key, dest_key in pairings):\n",
    "        return  # Do not write checkpoint if no need to repair!\n",
    "    out_state_dict = {}\n",
    "    for src_key, dest_key in pairings:\n",
    "        print(f\"{src_key}  ==>  {dest_key}\")\n",
    "        out_state_dict[dest_key] = in_state_dict[src_key]\n",
    "    # ckpt[\"model_state_dict\"] = out_state_dict\n",
    "    torch.save(out_state_dict, out_path)\n",
    "\n",
    "# repair_checkpoint('small/best_tune_llm.pt', 'small/best_tune_llm_fixed.pt')\n",
    "repair_checkpoint(f'{model_folder}/tune_llm_990.pt', f'{model_folder}/tune_llm_990_fixed.pt')\n",
    "\n",
    "model = Mllm(VOCAB_SIZE, BLOCK_SIZE, N_EMBED, N_HEAD, N_LAYERS)\n",
    "model.load_state_dict(torch.load(f'{model_folder}/tune_llm_190_fixed.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic quantization\n",
    "# model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "speak(model.to(device), tokenizer, \"Hi there! I'm Nicolas STAS \", max_len=1024, temperature=0.9)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model to onnx\n",
    "\n",
    "dummy_input = torch.zeros((1, BLOCK_SIZE), dtype=torch.int32).to('cuda')\n",
    "model = model.eval().to('cuda')\n",
    "# model = torch.jit.script(model)\n",
    "# onnx_prog = torch.onnx.dynamo_export(model, dummy_input)\n",
    "torch.onnx.export(model, dummy_input, f'{model_folder}/llm.onnx', opset_version=17, input_names=['input'], output_names=['output'], dynamic_axes={'input':{0:'batch_size', 1:'sequence'}, 'output':{0:'batch_size', 1:'sequence'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install onnxruntime onnx\n",
    "# import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, preprocess\n",
    "\n",
    "model_fp32 = f'{model_folder}/llm.onnx'\n",
    "model_quant = f'{model_folder}/llm.quant.onnx'\n",
    "preprocess_model = f'{model_folder}/llm.preprocess.onnx'\n",
    "# preprocess(model_fp32, model_quant, num_bits=8)\n",
    "# preprocess.quant_pre_process(model_fp32, preprocess_model)\n",
    "\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generate with onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "sess_options.enable_profiling = True\n",
    "sess = onnxruntime.InferenceSession(model_quant)\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "print(input_name, label_name)\n",
    "\n",
    "def genearte_onnx(sess, input_name, label_name, prompt, max_len=100, temperature=0.5, repetition = 1.1, device = 'cuda'):\n",
    "    input_ = tokenizer.tokenize(prompt)\n",
    "    input_ = torch.tensor(input_, dtype=torch.int32).unsqueeze(0).to(device)\n",
    "    gen = []\n",
    "    for _ in range(max_len):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = input_ if input_.size(1) <= BLOCK_SIZE else input_[:, -BLOCK_SIZE:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits = sess.run([label_name], {input_name: idx_cond.cpu().numpy()})[0]\n",
    "        logits = torch.tensor(logits, dtype=torch.float32).to(device)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        #  count token repetitions and apply repetition penalty\n",
    "        counts = Counter(input_.view(-1).tolist())\n",
    "        # avg = sum(counts.values()) / len(counts)\n",
    "        for k,v in counts.items():\n",
    "            logits[:,k] /= repetition**(v)\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        #  cast to int32\n",
    "        idx_next = idx_next.to(torch.int32)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        input_ = torch.cat((input_, idx_next), dim=1)\n",
    "        gen.append(idx_next.item())\n",
    "    return gen\n",
    "\n",
    "def speak_onnx(sess, input_name, label_name, prompt, max_len=100, temperature=0.5, repetition = 1.1, device = 'cuda'):\n",
    "    gen = genearte_onnx(sess, input_name, label_name, prompt, max_len, temperature, repetition, device)\n",
    "    text = tokenizer.detokenize(gen)\n",
    "    text = text.replace(' . ', '\\n')\n",
    "    print(prompt, text)\n",
    "\n",
    "speak_onnx(sess, input_name, label_name, \"Hi there! I'm Nicolas STAS \", max_len=2048)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first-order",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
